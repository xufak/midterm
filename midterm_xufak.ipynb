{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec9a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from langdetect import detect\n",
    "\n",
    "warnings. simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c47944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# there are many five star reviews, which can bias the classifier, so getting rid of some\n",
    "fives = df.loc[df['Score'] == 5]\n",
    "fives = fives.sample(frac=0.5)\n",
    "df = pd.concat([df.loc[df['Score'] != 5], fives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0a0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "# get relevant fields of data\n",
    "df['Text'].loc[df[\"Text\"].isna()] = \"\"\n",
    "df['Summary'].loc[df[\"Summary\"].isna()] = \"\"\n",
    "\n",
    "# check for different languages\n",
    "# df['Language'] = df['Text'].apply(lambda x: det_lan(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbbeabf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change emojis into words to parse\n",
    "df[\"Text\"] = df[\"Text\"].replace([\"\\:\\)\", \"\\:\\-\\)\", \"\\:\\-\\}\", \"\\;\\-\\}\", \"\\:\\-\\>\", \"\\;\\-\\)\"], [\"Happy\",\"Happy\",\"Happy\",\"Happy\",\"Happy\",\"Happy\"], regex=True)\n",
    "df[\"Text\"] = df[\"Text\"].replace([\"\\:\\-\\(\", \"\\:\\(\", \"\\:\\-\\|\", \"\\;\\-\\(\", \"\\;\\-\\<\", \"\\|\\-\\{\"], [\"Sad\", \"Sad\", \"Sad\", \"Sad\", \"Sad\", \"Sad\",], regex=True)\n",
    "df[\"Text\"] = df[\"Text\"].replace([\"\\:\\D\", \"\\:\\'\\-\\)\", \"\\:\\`\\-\\(\", \"\\>\\:\\(\", \"\\>\\:\\-\\(\"], [\"laugh\", \"tear of joy\", \"tear of sadness\", \"angry\", \"angry\"], regex=True)\n",
    "\n",
    "# remove punctuation\n",
    "df[\"Text\"] = df[\"Text\"].str.replace('[^\\w\\s]','')\n",
    "df[\"Summary\"] = df[\"Summary\"].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# separate test and train data\n",
    "df_train = df.loc[df['Score'].isna() == False]\n",
    "df_test = df.loc[df['Score'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ac4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Text'] = df_train['Text'].str.lower()\n",
    "df_test['Text']  = df_test['Text'].str.lower()\n",
    "df_train['Summary'] = df_train['Summary'].str.lower()\n",
    "df_test['Summary']  = df_test['Summary'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9159db23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>ADZPIG9QOCDG5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1203984000</td>\n",
       "      <td>good version of a classic</td>\n",
       "      <td>This is a charming version of the classic Dick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>A35947ZP82G7JH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1388361600</td>\n",
       "      <td>Good but not as moving</td>\n",
       "      <td>It was good but not as emotionally moving as t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>A3UORV8A9D5L2E</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1388361600</td>\n",
       "      <td>Winklers Performance was ok at best</td>\n",
       "      <td>Dont get me wrong Winkler is a wonderful chara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>A3R27T4HADWFFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1387670400</td>\n",
       "      <td>Best Scrooge yet</td>\n",
       "      <td>This is one of the best Scrooge movies out  He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0005019281</td>\n",
       "      <td>A2L0G56BNOTX6S</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1383696000</td>\n",
       "      <td>Dickens updated</td>\n",
       "      <td>This has been a favorite movie of mine for a l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId  HelpfulnessNumerator  \\\n",
       "0   0  0005019281   ADZPIG9QOCDG5                     0   \n",
       "1   1  0005019281  A35947ZP82G7JH                     0   \n",
       "2   2  0005019281  A3UORV8A9D5L2E                     0   \n",
       "4   4  0005019281  A3R27T4HADWFFJ                     0   \n",
       "5   5  0005019281  A2L0G56BNOTX6S                     0   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       0    4.0  1203984000   \n",
       "1                       0    3.0  1388361600   \n",
       "2                       0    3.0  1388361600   \n",
       "4                       0    4.0  1387670400   \n",
       "5                       0    NaN  1383696000   \n",
       "\n",
       "                               Summary  \\\n",
       "0            good version of a classic   \n",
       "1               Good but not as moving   \n",
       "2  Winklers Performance was ok at best   \n",
       "4                     Best Scrooge yet   \n",
       "5                      Dickens updated   \n",
       "\n",
       "                                                Text  \n",
       "0  This is a charming version of the classic Dick...  \n",
       "1  It was good but not as emotionally moving as t...  \n",
       "2  Dont get me wrong Winkler is a wonderful chara...  \n",
       "4  This is one of the best Scrooge movies out  He...  \n",
       "5  This has been a favorite movie of mine for a l...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02be8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df_train_tok = df_train.copy()\n",
    "df_train_tok[\"Text\"] = df_train[\"Text\"].apply(word_tokenize)\n",
    "df_train_tok[\"Summary\"] = df_train[\"Summary\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6811f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stop words from tokenized lists on each dataframe entry\n",
    "stop_words = list(stopwords.words('english')) #About 179 stopwords\n",
    "df_train_tok[\"Text\"] = df_train_tok[\"Text\"].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "df_train_tok[\"Summary\"] = df_train_tok[\"Summary\"].apply(lambda x: [item for item in x if item not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# stem tokenized words to avoid overfitting\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "df_train_tok['Text'] = df_train_tok['Text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df_train_tok['Summary'] = df_train_tok['Summary'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# count occurences of words to input that as data for KNN because it can only do computations on numbers\n",
    "print('vectorizer')\n",
    "count_w = HashingVectorizer()\n",
    "train_counts_sum = count_w.fit_transform(df['Summary'])\n",
    "train_counts_txt = count_w.fit_transform(df['Text'])\n",
    "\n",
    "# turn occurences into frequency\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts_sum)\n",
    "train_tfidf2 = tfidf_transformer.fit_transform(train_counts_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db512714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
    "\n",
    "count_w = TfidfVectorizer(max_df=0.9, min_df=0.05)\n",
    "str_df = df['Summary'].values + \" \" +  df['Text'].values\n",
    "train_counts_sum = count_w.fit_transform(str_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set\n",
    "submissionSet = pd.read_csv(\"./data/test.csv\")\n",
    "train_processed = df\n",
    "    \n",
    "# Merge on Id so that the test set can have feature columns as well\n",
    "testX= pd.merge(train_processed, submissionSet, left_on='Id', right_on='Id')\n",
    "testX = testX.drop(columns=['Score_x'])\n",
    "testX = testX.rename(columns={'Score_y': 'Score'})\n",
    "\n",
    "# The training set is where the score is not null\n",
    "trainX =  train_processed[train_processed['Score'].notnull()]\n",
    "\n",
    "testX.to_csv(\"./data/X_test.csv\", index=False)\n",
    "trainX.to_csv(\"./data/X_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load files into DataFrames\n",
    "X_train = pd.read_csv(\"./data/X_train.csv\")\n",
    "X_submission = pd.read_csv(\"./data/X_test.csv\")\n",
    "\n",
    "# Split training set into training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_train.drop(['Score'], axis=1),\n",
    "        X_train['Score'],\n",
    "        test_size=1/4.0,\n",
    "        random_state=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a13da6",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# This is where you can do more feature selection\n",
    "X_train_processed = X_train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_test_processed = X_test.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_submission_processed = X_submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'Score'])\n",
    "\n",
    "# Learn the model\n",
    "model = MultinomialNB().fit(X_train_processed, Y_train)\n",
    "\n",
    "# Predict the score using the model\n",
    "Y_test_predictions = model.predict(X_test_processed)\n",
    "X_submission['Score'] = model.predict(X_submission_processed)\n",
    "\n",
    "# Evaluate your model on the testing set\n",
    "print(\"Accuracy on testing set = \", accuracy_score(Y_test, Y_test_predictions))\n",
    "\n",
    "# Plot a confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211ebd3",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b324f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is where you can do more feature selection\n",
    "X_train_processed = X_train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_test_processed = X_test.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_submission_processed = X_submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'Score'])\n",
    "\n",
    "# Learn the model\n",
    "model = LogisticRegression().fit(X_train_processed, Y_train)\n",
    "\n",
    "# Predict the score using the model\n",
    "Y_test_predictions = model.predict(X_test_processed)\n",
    "X_submission['Score'] = model.predict(X_submission_processed)\n",
    "\n",
    "# Evaluate your model on the testing set\n",
    "print(\"Accuracy on testing set = \", accuracy_score(Y_test, Y_test_predictions))\n",
    "\n",
    "# Plot a confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a76f2",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# This is where you can do more feature selection\n",
    "X_train_processed = X_train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_test_processed = X_test.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_submission_processed = X_submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'Score'])\n",
    "\n",
    "# Learn the model\n",
    "model = DecisionTreeClassifier().fit(X_train_processed, Y_train)\n",
    "\n",
    "# Predict the score using the model\n",
    "Y_test_predictions = model.predict(X_test_processed)\n",
    "X_submission['Score'] = model.predict(X_submission_processed)\n",
    "\n",
    "# Evaluate your model on the testing set\n",
    "print(\"Accuracy on testing set = \", accuracy_score(Y_test, Y_test_predictions))\n",
    "\n",
    "# Plot a confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a35a9f",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ce39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# This is where you can do more feature selection\n",
    "X_train_processed = X_train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_test_processed = X_test.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_submission_processed = X_submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'Score'])\n",
    "\n",
    "# Learn the model\n",
    "model = ExtraTreesClassifier().fit(X_train_processed, Y_train)\n",
    "\n",
    "# Predict the score using the model\n",
    "Y_test_predictions = model.predict(X_test_processed)\n",
    "X_submission['Score'] = model.predict(X_submission_processed)\n",
    "\n",
    "# Evaluate your model on the testing set\n",
    "print(\"Accuracy on testing set = \", accuracy_score(Y_test, Y_test_predictions))\n",
    "\n",
    "# Plot a confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b29e6e",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# This is where you can do more feature selection\n",
    "X_train_processed = X_train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_test_processed = X_test.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
    "X_submission_processed = X_submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'Score'])\n",
    "\n",
    "# Learn the model\n",
    "model = SVC().fit(X_train_processed, Y_train)\n",
    "\n",
    "# Predict the score using the model\n",
    "Y_test_predictions = model.predict(X_test_processed)\n",
    "X_submission['Score'] = model.predict(X_submission_processed)\n",
    "\n",
    "# Evaluate your model on the testing set\n",
    "print(\"Accuracy on testing set = \", accuracy_score(Y_test, Y_test_predictions))\n",
    "\n",
    "# Plot a confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_test_predictions, normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the submission file\n",
    "submission = X_submission[['Id', 'Score']]\n",
    "submission.to_csv(\"./data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
